{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from os.path import dirname\n",
    "proj_path = dirname(os.getcwd())\n",
    "sys.path.append(proj_path)\n",
    "\n",
    "from typing import Mapping\n",
    "\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "\n",
    "from oatomobile.baselines.torch.dim.model import ImitativeModel\n",
    "from oatomobile.datasets.carla import CARLADataset\n",
    "from oatomobile.torch import types\n",
    "from oatomobile.torch.loggers import TensorBoardLogger\n",
    "from oatomobile.torch.savers import Checkpointer\n",
    "\n",
    "import numpy as np\n",
    "import ray.train as train\n",
    "from ray.train import Trainer, TorchConfig\n",
    "from ray.train.callbacks import JsonLoggerCallback, TBXLoggerCallback\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data import DistributedSampler\n",
    "\n",
    "import ray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_address = \"127.0.0.1\"\n",
    "num_workers = 4\n",
    "smoke_test = True\n",
    "dataset_dir = \"nonlocalexamples\"\n",
    "output_dir = \"models\"\n",
    "batch_size = 512\n",
    "num_epochs = 3\n",
    "save_model_frequency = 4\n",
    "learning_rate = 1e-3\n",
    "num_timesteps_to_keep = 4\n",
    "weight_decay = 0.0\n",
    "clip_gradients = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(confg):\n",
    "  batch_size = config.get(\"batch_size\", 32)\n",
    "  hidden_size = config.get(\"hidden_size\", 1)\n",
    "  lr = config.get(\"lr\", 1e-2)\n",
    "  epochs = config.get(\"epochs\", 3)\n",
    "  weight_decay = config.get(\"weight_decay\", 0)\n",
    "  # # Creates the necessary output directory.\n",
    "  # os.makedirs(output_dir, exist_ok=True)\n",
    "  # log_dir = os.path.join(output_dir, \"logs\")\n",
    "  # os.makedirs(log_dir, exist_ok=True)\n",
    "  # ckpt_dir = os.path.join(output_dir, \"ckpts\")\n",
    "  # os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Initializes the model and its optimizer.\n",
    "    output_shape = [num_timesteps_to_keep, 2]\n",
    "    model = ImitativeModel(output_shape=output_shape).to(device)\n",
    "    model = DistributedDataParallel(model)\n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "    # writer = TensorBoardLogger(log_dir=log_dir)\n",
    "    # checkpointer = Checkpointer(model=model, ckpt_dir=ckpt_dir)\n",
    "\n",
    "    def transform(batch: Mapping[str, types.Array]) -> Mapping[str, torch.Tensor]:\n",
    "    \"\"\"Preprocesses a batch for the model.\n",
    "\n",
    "    Args:\n",
    "      batch: (keyword arguments) The raw batch variables.\n",
    "\n",
    "    Returns:\n",
    "      The processed batch.\n",
    "    \"\"\"\n",
    "    # Sends tensors to `device`.\n",
    "    batch = {key: tensor.to(device) for (key, tensor) in batch.items()}\n",
    "    # Preprocesses batch for the model.\n",
    "    batch = model.transform(batch)\n",
    "    return batch\n",
    "\n",
    "  # Setups the dataset and the dataloader.\n",
    "  modalities = (\n",
    "      \"lidar\",\n",
    "      \"is_at_traffic_light\",\n",
    "      \"traffic_light_state\",\n",
    "      \"player_future\",\n",
    "      \"velocity\",\n",
    "  )\n",
    "  dataset_train = CARLADataset.as_torch(\n",
    "      dataset_dir=os.path.join(dataset_dir, \"train\"),\n",
    "      modalities=modalities,\n",
    "  )\n",
    "  dataloader_train = torch.utils.data.DataLoader(\n",
    "      dataset_train,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=5,\n",
    "  )\n",
    "  dataset_val = CARLADataset.as_torch(\n",
    "      dataset_dir=os.path.join(dataset_dir, \"val\"),\n",
    "      modalities=modalities,\n",
    "  )\n",
    "  dataloader_val = torch.utils.data.DataLoader(\n",
    "      dataset_val,\n",
    "      batch_size=batch_size * 5,\n",
    "      shuffle=True,\n",
    "      num_workers=5,\n",
    "  )\n",
    "\n",
    "  # Theoretical limit of NLL.\n",
    "  nll_limit = -torch.sum(  # pylint: disable=no-member\n",
    "      D.MultivariateNormal(\n",
    "          loc=torch.zeros(output_shape[-2] * output_shape[-1]),  # pylint: disable=no-member\n",
    "          scale_tril=torch.eye(output_shape[-2] * output_shape[-1]) *  # pylint: disable=no-member\n",
    "          noise_level,  # pylint: disable=no-member\n",
    "      ).log_prob(torch.zeros(output_shape[-2] * output_shape[-1])))  # pylint: disable=no-member\n",
    "\n",
    "  def train_step(\n",
    "      model: ImitativeModel,\n",
    "      optimizer: optim.Optimizer,\n",
    "      batch: Mapping[str, torch.Tensor],\n",
    "      clip: bool = False,\n",
    "  ) -> torch.Tensor:\n",
    "    \"\"\"Performs a single gradient-descent optimisation step.\"\"\"\n",
    "    # Resets optimizer's gradients.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Perturb target.\n",
    "    y = torch.normal(  # pylint: disable=no-member\n",
    "        mean=batch[\"player_future\"][..., :2],\n",
    "        std=torch.ones_like(batch[\"player_future\"][..., :2]) * noise_level,  # pylint: disable=no-member\n",
    "    )\n",
    "\n",
    "    # Forward pass from the model.\n",
    "    z, _ = model._params(\n",
    "        velocity=batch[\"velocity\"],\n",
    "        visual_features=batch[\"visual_features\"],\n",
    "        is_at_traffic_light=batch[\"is_at_traffic_light\"],\n",
    "        traffic_light_state=batch[\"traffic_light_state\"],\n",
    "    )\n",
    "    _, log_prob, logabsdet = model._decoder._inverse(y=y, z=z)\n",
    "\n",
    "    # Calculates loss (NLL).\n",
    "    loss = -torch.mean(log_prob - logabsdet, dim=0)  # pylint: disable=no-member\n",
    "\n",
    "    # Backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Clips gradients norm.\n",
    "    if clip:\n",
    "      torch.nn.utils.clip_grad_norm(model.parameters(), 1.0)\n",
    "\n",
    "    # Performs a gradient descent step.\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def train_epoch(\n",
    "      model: ImitativeModel,\n",
    "      optimizer: optim.Optimizer,\n",
    "      dataloader: torch.utils.data.DataLoader,\n",
    "  ) -> torch.Tensor:\n",
    "    \"\"\"Performs an epoch of gradient descent optimization on `dataloader`.\"\"\"\n",
    "    model.train()\n",
    "    loss = 0.0\n",
    "    with tqdm.tqdm(dataloader) as pbar:\n",
    "      for batch in pbar:\n",
    "        # Prepares the batch.\n",
    "        batch = transform(batch)\n",
    "        # Performs a gradien-descent step.\n",
    "        loss += train_step(model, optimizer, batch, clip=clip_gradients)\n",
    "    return loss / len(dataloader)\n",
    "\n",
    "  def evaluate_step(\n",
    "      model: ImitativeModel,\n",
    "      batch: Mapping[str, torch.Tensor],\n",
    "  ) -> torch.Tensor:\n",
    "    \"\"\"Evaluates `model` on a `batch`.\"\"\"\n",
    "    # Forward pass from the model.\n",
    "    z, _ = model._params(\n",
    "        velocity=batch[\"velocity\"],\n",
    "        visual_features=batch[\"visual_features\"],\n",
    "        is_at_traffic_light=batch[\"is_at_traffic_light\"],\n",
    "        traffic_light_state=batch[\"traffic_light_state\"],\n",
    "    )\n",
    "    _, log_prob, logabsdet = model._decoder._inverse(\n",
    "        y=batch[\"player_future\"][..., :2],\n",
    "        z=z,\n",
    "    )\n",
    "\n",
    "    # Calculates loss (NLL).\n",
    "    loss = -torch.mean(log_prob - logabsdet, dim=0)  # pylint: disable=no-member\n",
    "\n",
    "    return loss\n",
    "\n",
    "  def evaluate_epoch(\n",
    "      model: ImitativeModel,\n",
    "      dataloader: torch.utils.data.DataLoader,\n",
    "  ) -> torch.Tensor:\n",
    "    \"\"\"Performs an evaluation of the `model` on the `dataloader.\"\"\"\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    with tqdm.tqdm(dataloader) as pbar:\n",
    "      for batch in pbar:\n",
    "        # Prepares the batch.\n",
    "        batch = transform(batch)\n",
    "        # Accumulates loss in dataset.\n",
    "        with torch.no_grad():\n",
    "          loss += evaluate_step(model, batch)\n",
    "    return loss / len(dataloader)\n",
    "\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        # Trains model on whole training dataset, and writes on `TensorBoard`.\n",
    "        loss_train = train_epoch(model, optimizer, dataloader_train)\n",
    "        # write(model, dataloader_train, writer, \"train\", loss_train, epoch)\n",
    "\n",
    "        # Evaluates model on whole validation dataset, and writes on `TensorBoard`.\n",
    "        loss_val = evaluate_epoch(model, dataloader_val)\n",
    "        # write(model, dataloader_val, writer, \"val\", loss_val, epoch)\n",
    "\n",
    "        # # Checkpoints model weights.\n",
    "        # if epoch % save_model_frequency == 0:\n",
    "        #     checkpointer.save(epoch)\n",
    "\n",
    "        # train_epoch(train_loader, model, loss_fn, optimizer)\n",
    "        # result = validate_epoch(validation_loader, model, loss_fn)\n",
    "        train.report(**result)\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_carla():\n",
    "    trainer = Trainer(TorchConfig(backend=\"gloo\"), num_workers=num_workers)\n",
    "    config = {\"lr\": learning_rate, \"hidden_size\": 1, \"batch_size\": batch_size, \"epochs\": num_epochs}\n",
    "    trainer.start()\n",
    "    results = trainer.run(\n",
    "        train_func,\n",
    "        config,\n",
    "        callbacks=[JsonLoggerCallback(),\n",
    "                   TBXLoggerCallback()])\n",
    "    trainer.shutdown()\n",
    "\n",
    "    print(results)\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Start Ray Cluster\n",
    "    if smoke_test:\n",
    "        ray.init(num_cpus=2)\n",
    "    else:\n",
    "        ray.init(address=address)\n",
    "    # Train carla\n",
    "    train_carla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0c060b2a3575904847ee0539dc2ac171628bacad6a3b01483d90c97ee6179c16"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('venv-3.7': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
