{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References \n",
    "- [Ray Train](https://docs.ray.io/en/latest/train/train.html#)\n",
    "- [Tensorboard & Pytorch](https://pytorch.org/docs/stable/tensorboard.html)\n",
    "\n",
    "Now let’s convert this to a distributed multi-worker training function!\n",
    "\n",
    "We keep the model unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "num_samples = 20\n",
    "input_size = 10\n",
    "layer_size = 15\n",
    "output_size = 5\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layer2(self.relu(self.layer1(input)))\n",
    "\n",
    "# In this example we use a randomly generated dataset.\n",
    "input = torch.randn(num_samples, input_size)\n",
    "labels = torch.randn(num_samples, output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, update the training function code to use PyTorch’s **DistributedDataParallel**. With Ray Train, you just pass in your distributed data parallel code as as you would normally run it with torch.distributed.launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import ray.train as train\n",
    "\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "def train_func():\n",
    "    num_epochs = 3\n",
    "    model = NeuralNetwork()\n",
    "    # Add graph to tensorboard, default goto ./runs\n",
    "    # writer.add_graph(model, input)\n",
    "    model = DistributedDataParallel(model)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        rank = train.world_rank()\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train.report(loss=loss.item())\n",
    "        train.save_checkpoint(epoch=f\"{rank-epoch}\", model=model.module)\n",
    "        print(f\"rank: {train.world_rank()}, epoch: {epoch}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, instantiate a Trainer that uses a \"torch\" backend with 4 workers, and use it to run the new training function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import Trainer\n",
    "\n",
    "logdir = \"raylog\"\n",
    "trainer = Trainer(backend=\"torch\", logdir=logdir, num_workers=4)\n",
    "# trainer.create_logdir(logdir)\n",
    "# trainer.create_run_dir()\n",
    "trainer.start()\n",
    "results = trainer.run(train_func)\n",
    "trainer.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Ray Train is Correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train\n",
    "from ray.train import Trainer\n",
    "\n",
    "def train_func(config):\n",
    "    model = 0 # This should be replaced with a real model.\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model += epoch\n",
    "        print(f\"rank: {train.world_rank()}; epoch: {epoch}; model: {model}\")\n",
    "        train.save_checkpoint(epoch=epoch, model=model)\n",
    "\n",
    "trainer = Trainer(backend=\"torch\", num_workers=2, logdir=\"raylog\")\n",
    "trainer.start()\n",
    "trainer.run(train_func, config={\"num_epochs\": 5})\n",
    "trainer.shutdown()\n",
    "\n",
    "print(trainer.latest_checkpoint)\n",
    "# {'epoch': 4, 'model': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import train\n",
    "from ray.train import Trainer\n",
    "\n",
    "def train_func(config):\n",
    "    checkpoint = train.load_checkpoint() or {}\n",
    "    print(checkpoint)\n",
    "    # This should be replaced with a real model.\n",
    "    model = checkpoint.get(\"model\")\n",
    "    start_epoch = checkpoint.get(\"epoch\") + 1\n",
    "    for epoch in range(start_epoch, config[\"num_epochs\"]):\n",
    "        model += epoch\n",
    "        train.save_checkpoint(epoch=epoch, model=model)\n",
    "\n",
    "trainer = Trainer(backend=\"torch\", num_workers=1)\n",
    "trainer.start()\n",
    "print(\"Model 1:\")\n",
    "trainer.run(train_func, config={\"num_epochs\": 5},\n",
    "            checkpoint=\"~/ray_results/raylog/run_001/checkpoints/checkpoint_000001\")\n",
    "print(\"Model 2:\")\n",
    "trainer.run(train_func, config={\"num_epochs\": 5},\n",
    "            checkpoint=\"~/ray_results/raylog/run_001/checkpoints/checkpoint_000002\")\n",
    "print(\"Model 3:\")\n",
    "trainer.run(train_func, config={\"num_epochs\": 5},\n",
    "            checkpoint=\"~/ray_results/raylog/run_001/checkpoints/checkpoint_000003\")\n",
    "print(\"Model 4:\")\n",
    "trainer.run(train_func, config={\"num_epochs\": 5},\n",
    "            checkpoint=\"~/ray_results/raylog/run_001/checkpoints/checkpoint_000004\")\n",
    "print(\"Model 5:\")\n",
    "trainer.run(train_func, config={\"num_epochs\": 5},\n",
    "            checkpoint=\"~/ray_results/raylog/run_001/checkpoints/checkpoint_000005\")\n",
    "trainer.shutdown()\n",
    "\n",
    "print(f\"Final model we are expecting is {trainer.latest_checkpoint}\")\n",
    "# {'epoch': 4, 'model': 10}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65cdd5b4a0e89948390188008d4fb68a6c5e0d3e09a99d6a4df67b3be73eed40"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('3.7': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
