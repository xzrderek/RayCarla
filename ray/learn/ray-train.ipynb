{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References \n",
    "- [Ray Train](https://docs.ray.io/en/latest/train/train.html#)\n",
    "- [Tensorboard & Pytorch](https://pytorch.org/docs/stable/tensorboard.html)\n",
    "\n",
    "Now let’s convert this to a distributed multi-worker training function!\n",
    "\n",
    "We keep the model unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "num_samples = 20\n",
    "input_size = 10\n",
    "layer_size = 15\n",
    "output_size = 5\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.layer2(self.relu(self.layer1(input)))\n",
    "\n",
    "# In this example we use a randomly generated dataset.\n",
    "input = torch.randn(num_samples, input_size)\n",
    "labels = torch.randn(num_samples, output_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, update the training function code to use PyTorch’s **DistributedDataParallel**. With Ray Train, you just pass in your distributed data parallel code as as you would normally run it with torch.distributed.launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "# Writer will output to ./runs/ directory by default\n",
    "writer = SummaryWriter()\n",
    "\n",
    "\n",
    "def train_func():\n",
    "    num_epochs = 3\n",
    "    model = NeuralNetwork()\n",
    "    # Add graph to tensorboard, default goto ./runs\n",
    "    # writer.add_graph(model, input)\n",
    "    model = DistributedDataParallel(model)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"epoch: {epoch}, loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, instantiate a Trainer that uses a \"torch\" backend with 4 workers, and use it to run the new training function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import Trainer\n",
    "\n",
    "trainer = Trainer(backend=\"torch\", num_workers=4)\n",
    "trainer.start()\n",
    "results = trainer.run(train_func)\n",
    "trainer.shutdown()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65cdd5b4a0e89948390188008d4fb68a6c5e0d3e09a99d6a4df67b3be73eed40"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('3.7': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
